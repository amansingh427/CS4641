{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T17:17:25.168179Z",
     "start_time": "2019-06-20T17:17:24.986130Z"
    },
    "colab_type": "text",
    "id": "Hjp__3bRh42K",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Fall 2020 CS4641 B Homework 4\n",
    "\n",
    "## Instructor: Rodrigo Borela Valente\n",
    "\n",
    "## Deadline: November 11th, Wednesday, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Piazza as part of the Q/A. However, all assignments should be done individually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4 Structure\n",
    "\n",
    "Homework 4 will have two components to it: \n",
    "* Programming (to be submitted to \"A4 programming\")\n",
    "* Theory (to be submitted to \"A4 Written\")\n",
    "* Bonus (to be submitted to \"A4 bonus\")\n",
    "\n",
    "**Note** - There will be 1 more gradescope folder\n",
    "1. For Early bird submission (Part 1 Programming)\n",
    "\n",
    "**Note**\n",
    "- Early Bird Special: If you can submit Part 1 by November 2nd, you get a bonus of 1 point for this assignment.\n",
    "- Irrespective of whether you submit an answer for early bird, submit it again in A4 programming for the final submission.\n",
    "\n",
    "**Note** - Points for each question are mentioned in their respective cells and the questions are present in this ipynb file.\n",
    "There is programming portion in this Jupyter notebook along with the theory questions. The homework is worth a total of 100 points and 30 bonus points. The grading breakdown is as follows:\n",
    " \n",
    "1. Programming (80 pts)\n",
    "    - Part 1, Part 2, Part 3, Part 4.3\n",
    "2. Theory (20 pts):\n",
    "    - 4.1, 4.2\n",
    "3. Bonus (30 pts)\n",
    "    - Part 5\n",
    "\n",
    "**Total = 100 pts + (30 pts bonus)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSJJRXYL2yOY"
   },
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "- We will be using Gradescope for submission and grading.\n",
    "\n",
    "- For the written (non-programming) part of the assignment\n",
    "    - Your write up must be submitted in PDF form, you may use either Latex or markdown, whichever you prefer. We will **not accept hand-written work**.\n",
    "    - If you are using a latex/markdown editor, please make sure to start answering each question on a new page. It makes it more organized to map your answers on GradeScope. When submitting your assignment, you must **correctly map pages** of your PDF to each question/subquestion where they appear. Improperly mapped questions may not be graded correctly.\n",
    "    - For part 4.3, attach the pdf of jupyter notebook to the written part of the assignment.\n",
    "\n",
    "\n",
    "- To create a pdf of your jupyter notebook, on the top left hand corner click on \"File\"->\"Download as\"->\"pdf (.pdf)\". \n",
    "\n",
    "- When uploading the pdf for the written portion of the assignment, please map the questions to pages were your solution begins (as was done for assignment 1 and 2).\n",
    "\n",
    "- Submit the programming solutions to their respective folders on gradescope as per the instructions given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nN-9x-Qm2yOY"
   },
   "source": [
    "### Using the autograder\n",
    "\n",
    "- You will submit your code (.py files) for the autograder in the following folders:\n",
    "\n",
    "    - \"A4 Programming\":\n",
    "        * util.py\n",
    "        * decision_tree.py\n",
    "        * random_forest.py\n",
    "        * nn.py\n",
    "        \n",
    "    - \"A4 Early Bird\":\n",
    "        * util.py\n",
    "\n",
    "- All you will have to do is to copy your implementations of the classes \"util\", \"DecisionTree\", \"RandomForest\", \"dlnet\" onto the respective files. We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n",
    "\n",
    "- **For the \"HW4 - Non-programming\" part, you will create a pdf (as was done for the theory portion of assignment 1 and 2) and submit it on Gradescope in A4 Written.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7U0WVt07tGRv"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0Ui6T2as9iI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amansingh/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from math import log2, sqrt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets.samples_generator import make_blobs \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mq4QZ4su2yOd"
   },
   "source": [
    "# Part 1: Utility Functions (15 pts)\n",
    "\n",
    "## Part 1.1: Evaluation Utility Functions\n",
    "\n",
    "Here, we ask you to develop a few functions that will be the main building blocks of your decision tree and random forest algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e_A6GBe11nA"
   },
   "source": [
    "### Entropy and information gain [6pts]\n",
    "\n",
    "First, we define and implement a function that computes entropy of the data.\\\n",
    "Then use this entropy function to compute the information gain for the partitioned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgbtf68v2yOe"
   },
   "source": [
    "## Part 1.2: Splitting Utility Functions\n",
    "\n",
    "Building a decision tree requires us to evaluate the best feature and value to split a node on. Now we will implement functions that help us determine these splits for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rp1CKXFW2yOf"
   },
   "source": [
    "#### (1) partition_classes: [3pts]\n",
    "\n",
    "One of the basic operations is to split a tree on one attribute (features) with a specific value for that attribute.\n",
    "\n",
    "In partition_classes(), we split the data (X) and labels (y) based on the split feature and value - BINARY SPLIT.\n",
    "\n",
    "You will have to first check if the split attribute is numerical or categorical. If the split attribute is numeric, split_val should be a numerical value. For example, your split_val should go over all the values of attributes. If the split attribute is categorical, split_val should include all the categories one by one.   \n",
    "    \n",
    "You can perform the partition in the following way:\n",
    "   - Numeric Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all\n",
    "       the rows where the split attribute is less than or equal to the split value, and the \n",
    "       second list has all the rows where the split attribute is greater than the split \n",
    "       value. Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "    \n",
    "   - Categorical Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all \n",
    "       the rows where the split attribute is equal to the split value, and the second list\n",
    "       has all the rows where the split attribute is not equal to the split value.\n",
    "       Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "\n",
    "Hint: You could find out if the feature is categorical by checking if it is the instance of 'str'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b70Z8KRR2yOf"
   },
   "source": [
    "#### (2) find_best_split [3pts]\n",
    "\n",
    "Given the data and labels, we need to find the order of splitting features, which is also the importance of the feature. For each attribute (feature), we need to calculate its optimal split value along with the corresponding information gain and then compare with all the features to find the optimal attribute to split.\n",
    "\n",
    "First, we specify an attribute. After computing the corresponding information gain of each value at this attribute list, we can get the optimal split value, which has the maximum information gain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t8lY7dZ2yOf"
   },
   "source": [
    "#### (3)  find_best_feature [3pts]\n",
    "\n",
    "Based on the above functions, we can find the most important feature that we will split first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVcXl9-D2DMW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log2, sqrt\n",
    "\n",
    "def entropy(class_y):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        - class_y: list of class labels (0's and 1's)\n",
    "    Output:\n",
    "        - entropy: a scalar, the value of entropy.\n",
    "    TODO:     [3 points]\n",
    "    \n",
    "    Compute the entropy for a list of classes\n",
    "    Example: entropy([0,0,0,1,1,1,1,1]) = 0.9544\n",
    "    \"\"\"\n",
    "    length = len(class_y)\n",
    "    if length == 0:\n",
    "        return 0\n",
    "    nz = np.count_nonzero(class_y)\n",
    "    p0 = (length - nz) / length\n",
    "    p1 = nz / length\n",
    "    if p0 == 0:\n",
    "        return p1 * log2(1 / p1)\n",
    "    elif p1 == 0:\n",
    "        return p0 * log2(1 / p0)\n",
    "    return p0 * log2(1 / p0) + p1 * log2(1 / p1)\n",
    "    \n",
    "\n",
    "def information_gain(previous_y, current_y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - previous_y : the distribution of original labels (0's and 1's)\n",
    "        - current_y  : the distribution of labels after splitting based on a particular\n",
    "                     split attribute and split value\n",
    "    Output:\n",
    "        - information_gain: a scalar, the value of information_gain.\n",
    "    \n",
    "    TODO:     [3 points]\n",
    "    \n",
    "    Compute and return the information gain from partitioning the previous_y labels into the current_y labels.\n",
    "    \n",
    "    Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf \n",
    "\n",
    "    Example: previous_y = [0,0,0,1,1,1], current_y = [[0,0], [1,1,1,0]], info_gain = 0.4591\n",
    "    \"\"\" \n",
    "    length = len(previous_y)\n",
    "    return entropy(previous_y) - (len(current_y[0]) / length * entropy(current_y[0]) + len(current_y[1]) / length * entropy(current_y[1]))\n",
    "\n",
    "def partition_classes(X, y, split_attribute, split_val):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X               : (N,D) list containing all data attributes\n",
    "    - y               : a list of labels\n",
    "    - split_attribute : column index of the attribute to split on\n",
    "    - split_val       : either a numerical or categorical value to divide the split_attribute\n",
    "    \n",
    "    Outputs:\n",
    "        - X_left, X_right, y_left, y_right : see the example below.\n",
    "        \n",
    "    TODO:    [3 points] \n",
    "    \n",
    "    Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X = [[3, 'aa', 10],                 y = [1,\n",
    "         [1, 'bb', 22],                      1,\n",
    "         [2, 'cc', 28],                      0,\n",
    "         [5, 'bb', 32],                      0,\n",
    "         [4, 'cc', 32]]                      1]\n",
    "    \n",
    "    Here, columns 0 and 2 represent numeric attributes, while column 1 is a categorical attribute.\n",
    "    \n",
    "    Consider the case where we call the function with split_attribute = 0 (the index of attribute) and split_val = 3 (the value of attribute).\n",
    "    Then we divide X into two lists - X_left, where column 0 is <= 3 and X_right, where column 0 is > 3.\n",
    "    \n",
    "    X_left = [[3, 'aa', 10],                 y_left = [1,\n",
    "              [1, 'bb', 22],                           1,\n",
    "              [2, 'cc', 28]]                           0]\n",
    "              \n",
    "    X_right = [[5, 'bb', 32],                y_right = [0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "\n",
    "    Consider another case where we call the function with split_attribute = 1 and split_val = 'bb'\n",
    "    Then we divide X into two lists, one where column 1 is 'bb', and the other where it is not 'bb'.\n",
    "        \n",
    "    X_left = [[1, 'bb', 22],                 y_left = [1,\n",
    "              [5, 'bb', 32]]                           0]\n",
    "              \n",
    "    X_right = [[3, 'aa', 10],                y_right = [1,\n",
    "               [2, 'cc', 28],                           0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "               \n",
    "               \n",
    "    Return in this order: X_left, X_right, y_left, y_right       \n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.array(X, dtype=object)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    # Both list and numpy arrays are allowed in util functions. However, the dataset in the parts below is# \n",
    "    # imported as numpy array. Therefore, we strongly recommend implementing as numpy array to make sure  #\n",
    "    # the autograder is stable. It will also reduce the run time for decision tree and random forest.     #    \n",
    "    # So please keep the lines above.                                                                     #\n",
    "    #######################################################################################################\n",
    "\n",
    "    arr = np.concatenate((X, y[:,None]), axis=1)\n",
    "    if isinstance(split_val, str):\n",
    "        left = arr[arr[:,split_attribute] == split_val]\n",
    "        right = arr[arr[:,split_attribute] != split_val]\n",
    "        X_left = np.array([a[:-1] for a in left])\n",
    "        X_right = np.array([a[:-1] for a in right])\n",
    "        y_left = np.array([a[-1] for a in left])\n",
    "        y_right = np.array([a[-1] for a in right])\n",
    "    else:\n",
    "        left = arr[arr[:,split_attribute] <= split_val]\n",
    "        right = arr[arr[:,split_attribute] > split_val]\n",
    "        X_left = np.array([a[:-1] for a in left])\n",
    "        X_right = np.array([a[:-1] for a in right])\n",
    "        y_left = np.array([a[-1] for a in left])\n",
    "        y_right = np.array([a[-1] for a in right])\n",
    "    return X_left, X_right, y_left, y_right\n",
    "\n",
    "def find_best_split(X, y, split_attribute):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - X               : (N,D) list containing all data attributes\n",
    "        - y               : a list array of labels\n",
    "        - split_attribute : Column of X on which to split\n",
    "    Outputs:\n",
    "        - best_split_val, info_gain : see the example below.\n",
    "        \n",
    "    TODO:    [3 points] \n",
    "    \n",
    "    Compute and return the optimal split value for a given attribute, along with the corresponding information gain\n",
    "    \n",
    "    Note: You will need the functions information_gain and partition_classes to write this function. \n",
    "    It is recommended that when dealing with numerical values, instead of discretizing the variable space, that you loop over the unique values in your dataset \n",
    "    (Hint: np.unique is your friend)\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           split_val = 1  -->  info_gain = 0.17\n",
    "           split_val = 2  -->  info_gain = 0.01997\n",
    "           split_val = 3  -->  info_gain = 0.01997\n",
    "           split_val = 4  -->  info_gain = 0.32\n",
    "           split_val = 5  -->  info_gain = 0\n",
    "        \n",
    "       best_split_val = 4; info_gain = .32; \n",
    "    \"\"\"\n",
    "    X = np.array(X, dtype = object)\n",
    "    \n",
    "    best_split_val, info_gain = 0, 0\n",
    "    for val in np.unique([a[split_attribute] for a in X]):\n",
    "        x_left, x_right, y_left, y_right = partition_classes(X, y, split_attribute, val)\n",
    "        ig = information_gain(y, [y_left, y_right])\n",
    "        if ig > info_gain:\n",
    "            info_gain = ig\n",
    "            best_split_val = val\n",
    "    return best_split_val, info_gain\n",
    "    \n",
    "    \n",
    "def find_best_feature(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - X: (N,D) list containing all data attributes\n",
    "        - y : a list of labels\n",
    "    \n",
    "    Outputs:\n",
    "        - best_split_feature, best_split_val: see the example below.\n",
    "    \n",
    "    TODO:    [3 points] \n",
    "    \n",
    "    Compute and return the optimal attribute to split on and optimal splitting value\n",
    "    \n",
    "    Note: If two features tie, choose one of them at random\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           feature 0:  -->  info_gain = 0.32\n",
    "           feature 1:  -->  info_gain = 0.17\n",
    "           feature 2:  -->  info_gain = 0.4199\n",
    "        \n",
    "       best_split_feature: 2 best_split_val: 22\n",
    "    \"\"\"\n",
    "    X = np.array(X, dtype = object)\n",
    "    \n",
    "    best_split_feature = 0\n",
    "    best_split_val = 0\n",
    "    info_gain = 0\n",
    "    for bsf in range(len(X[0])):\n",
    "        bsv, ig = find_best_split(X, y, bsf)\n",
    "        if ig > info_gain:\n",
    "            info_gain = ig\n",
    "            best_split_feature = bsf\n",
    "            best_split_val = bsv\n",
    "    return best_split_feature, best_split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-nbf6IJ2fkS",
    "outputId": "def19def-732b-498c-9415-78143c9e5d31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct entropy: 0.8812908992306927\n",
      "my entropy: 0.8812908992306926\n",
      "correct information gain: 0.5216406363433185\n",
      "my information gain: 0.5216406363433184\n"
     ]
    }
   ],
   "source": [
    "# Autograder Test Case\n",
    "# Helper function. Please do not modify.\n",
    "test_class_y = [0,0,0,1,1,1,1,1,1,1]\n",
    "print('correct entropy:', 0.8812908992306927)\n",
    "print('my entropy:',entropy(test_class_y))\n",
    "\n",
    "previous_y = [0,0,0,0,1,1,1]\n",
    "current_y = [[0,0,0], [1,1,1,0]] \n",
    "print('correct information gain:', 0.5216406363433185)\n",
    "print('my information gain:',information_gain(previous_y, current_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DlHFnPF92yOp",
    "outputId": "3b105321-a8c3-498b-ce74-0a2a16b364a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[3, 'dd', 23],\n",
      "       [1, 'dd', 8],\n",
      "       [4, 'bb', 32],\n",
      "       [2, 'cc', 15]], dtype=object), array([[6, 'bb', 13],\n",
      "       [8, 'aa', 31]], dtype=object), array([1, 0, 0, 1]), array([1, 1]))\n",
      "(array([[2, 'cc', 15]], dtype=object), array([[6, 'bb', 13],\n",
      "       [3, 'dd', 23],\n",
      "       [1, 'dd', 8],\n",
      "       [4, 'bb', 32],\n",
      "       [8, 'aa', 31]], dtype=object), array([1]), array([1, 1, 0, 0, 1]))\n",
      "best_split_val: 1 info_gain: 0.31668908831502074\n",
      "best_split_feature: 0 best_split_val: 1\n"
     ]
    }
   ],
   "source": [
    "#Autograder Test Case\n",
    "# Helper function. Please do not modify.\n",
    "test_X = [[6, 'bb', 13],[3, 'dd', 23],[1, 'dd', 8],[4, 'bb', 32],[8, 'aa', 31],[2, 'cc', 15]]\n",
    "test_y = [1,1,0,0,1,1]\n",
    "test_X, test_y = np.array(test_X, dtype='object'), np.array(test_y)\n",
    "print(partition_classes(test_X, test_y, 0, 4))\n",
    "print(partition_classes(test_X, test_y, 1, 'cc'))\n",
    "\n",
    "split_attribute = 0\n",
    "best_split_val, info_gain = find_best_split(test_X, test_y, split_attribute)\n",
    "print(\"best_split_val:\", best_split_val, \"info_gain:\", info_gain)\n",
    "\n",
    "best_feature, best_split_val = find_best_feature(test_X, test_y)\n",
    "print(\"best_split_feature:\", best_feature, \"best_split_val:\", best_split_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayv_tqnyxMb0"
   },
   "source": [
    "# Part 2: Decision Tree (25 pts)\n",
    "## Please read the following instructions carefully before you dive into coding\n",
    "\n",
    "In this part, you will implement your own ID3 decision tree class and make it work on training and test set.\n",
    "\n",
    "You may use a recursive way to construct the tree and make use of utility functions in Part1. \n",
    "\n",
    "Please keep in mind that we use information gain to find the best feature and value to split the data for ID3 tree.\n",
    "\n",
    "To save your training time, we have added a ```max_depth``` parameter to control the maximum depth of the tree. You may adjust its value to pre-prune the tree. If set to None, it has no control of depth.\n",
    "\n",
    "You need to have a stop condition for splitting. The stopping condtion is reached when one of the two following conditions are met:\n",
    "1. If all data points in that node have the same label\n",
    "2. If the current node is at the maximum depth. In this case, you may assign the mode of the labels as the class label\n",
    "\n",
    "The MyDecisionTree class should have some member variables. We highly encourage you to use a dict in Python to store the tree information. Here we initialize the tree as a dict for you. For leaves nodes, this dict may have just one element representing the class label. For non-leaves node, the list should at least store the feature and value to split, and references to its left and right child.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2Cwef24xgtT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "# from util import *\n",
    "\n",
    "\n",
    "class MyDecisionTree(object):\n",
    "    def __init__(self, max_depth=30):\n",
    "        \"\"\"\n",
    "        Helper Function: \n",
    "        Initializing the tree as an empty dictionary.\n",
    "\n",
    "        Args:      \n",
    "        max_depth: maximum depth of the tree including the root node.\n",
    "        Please consider the root node as being in depth = 0. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.tree = {\n",
    "            'isLeaf': False,\n",
    "            'split_attribute': -1,\n",
    "            'split_value': '',\n",
    "            'class_value': 0,\n",
    "            'is_categorical': False,\n",
    "            'leftTree': None,\n",
    "            'rightTree': None,\n",
    "            'depth': 0\n",
    "        };\n",
    "\n",
    "        \n",
    "    def fit(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        TODO:        [15 points] \n",
    "        \n",
    "        Train the decision tree (self.tree) using the the sample X and labels y.\n",
    "        \n",
    "        NOTE: You will have to make use of the utility functions to train the tree.\n",
    "        One possible way of implementing the tree: Each node in self.tree could be in the form of a dictionary:\n",
    "        https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        \n",
    "        For example, a non-leaf node with two children can have a 'left' key and  a  'right' key. \n",
    "        You can add more keys which might help in classification (eg. split attribute and split value)\n",
    "        \n",
    "        \n",
    "        While fitting a tree to the data, you will need to check to see if the node is a leaf node(\n",
    "        based on the stopping condition explained above) or not. \n",
    "        If it is not a leaf node, find the best feature and attribute split:\n",
    "        X_left, X_right, y_left, y_right, for the data to build the left and\n",
    "        the right subtrees.\n",
    "        \n",
    "        Remember for building the left subtree, pass only X_left and y_left and for the right subtree,\n",
    "        pass only X_right and y_right.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        X: N*D matrix corresponding to the data points\n",
    "        Y: N*1 array corresponding to the labels of the data points\n",
    "        depth: depth of node of the tree\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Hint: the structure of fit function is provided for you here. You have to \n",
    "        # implement the buildTree function by yourself to recursively build and add nodes to decision tree.\n",
    "        \n",
    "        X = X.astype('object')\n",
    "     \n",
    "        self.tree = self.buildTree(X, y, 0)\n",
    "        self.tree = self.tree.tree\n",
    "        \n",
    "    \n",
    "    def buildTree(self, X, y, depth):\n",
    "        \"\"\"\n",
    "            Recursively build and add nodes\n",
    "            \n",
    "        \"\"\"\n",
    "        t = MyDecisionTree(self.max_depth)\n",
    "        # check if we need to stop splitting\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            t.tree = {\n",
    "                'isLeaf': True,\n",
    "                'split_attribute': -1,\n",
    "                'split_value': '',\n",
    "                'class_value': 1 if sum(y) == len(y) else 0,\n",
    "                'is_categorical': False,\n",
    "                'leftTree': None,\n",
    "                'rightTree': None,\n",
    "                'depth': depth + 1\n",
    "            };\n",
    "            return t\n",
    "        # find best feature and attribute\n",
    "        best_split_feature, best_split_val = find_best_feature(X, y)\n",
    "        X_left, X_right, y_left, y_right = partition_classes(X, y, best_split_feature, best_split_val)\n",
    "        t.tree = {\n",
    "                'isLeaf': False,\n",
    "                'split_attribute': best_split_feature,\n",
    "                'split_value': best_split_val,\n",
    "                'class_value': 0,\n",
    "                'is_categorical': isinstance(best_split_val, str),\n",
    "                'leftTree': t.buildTree(X_left, y_left, depth),\n",
    "                'rightTree': t.buildTree(X_right, y_right, depth),\n",
    "                'depth': depth + 1\n",
    "            };\n",
    "        return t\n",
    "\n",
    "    def predict(self, record):\n",
    "        \"\"\"\n",
    "        TODO:        [10 points] \n",
    "        \n",
    "        classify a sample in test data set using self.tree and return the predicted label\n",
    "       \n",
    "        Args:\n",
    "        \n",
    "        record: D*1, a single data point that should be classified\n",
    "        \n",
    "        Returns: True if the predicted class label is 1, False otherwise      \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        node = self.tree\n",
    "        while not node['isLeaf']:\n",
    "            if node['is_categorical']:\n",
    "                if record[node['split_attribute']] == node['split_value']:\n",
    "                    node = node['leftTree'].tree\n",
    "                else:\n",
    "                    node = node['rightTree''].tree\n",
    "            else:\n",
    "                if record[node['split_attribute']] > node['split_value']:\n",
    "                    node = node['rightTree'].tree\n",
    "                else:\n",
    "                    node = node['leftTree'].tree\n",
    "        return True if node['class_value'] == 1 else False\n",
    "\n",
    "        \n",
    "    def DecisionTreeEvalution(self,X,y, verbose=False):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Make predictions\n",
    "        # For each test sample X, use our fitting dt classifer to predict\n",
    "        y_predicted = []\n",
    "        for record in X: \n",
    "            y_predicted.append(self.predict(record))\n",
    "\n",
    "        # Comparing predicted and true labels\n",
    "        results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True)) / float(len(results))\n",
    "        if verbose:\n",
    "            print(\"accuracy: %.4f\" % accuracy)\n",
    "        return accuracy\n",
    "\n",
    "    def DecisionTreeError(self, y):\n",
    "        # helper function for calculating the error of the entire subtree if converted to a leaf with majority class label.\n",
    "        # You don't have to modify it  \n",
    "        num_ones = np.sum(y)\n",
    "        num_zeros = len(y) - num_ones\n",
    "        return 1.0 - max(num_ones, num_zeros) / float(len(y))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5wmr3mt2yOu"
   },
   "source": [
    "### Dataset Objective\n",
    "\n",
    "We are the founders of a new e-commerce company that uses machine learning to optimize the user experience. We are tasked with the responsibility of coming up with a method for determining the likelihood of a shopping session ending in a purchase being made. We will then use this information to adjust pricing and services to encourage more purchasing.\n",
    "\n",
    "After much deliberation amongst the team, you come to a conclusion that we can use past online shopping data to predict the future occurence of revenue sessions. \n",
    "\n",
    "Our task is to use the decision tree algorithm to predict if a shopping session ends in a purchase.\n",
    "\n",
    "You can find more information on the dataset [here](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxDuP6Yq2yOv"
   },
   "source": [
    "### Loading the dataset\n",
    "\n",
    "\n",
    "The dataset that the company has collected has the following features:\n",
    "\n",
    "1. Administrative : continuous variable\n",
    "2. Administrative_Duration\t: continuous variable\n",
    "3. Informational : continuous variable\n",
    "4. Informational_Duration : continuous variable\n",
    "5. ProductRelated : continuous variable\n",
    "6. ProductRelated_Duration : continuous variable\n",
    "7. BounceRates : continuous variable\n",
    "8. ExitRates : continuous variable\n",
    "9. PageValues : continuous variable\n",
    "10. SpecialDay : continuous variable\n",
    "11. Month\t: categorical variable\n",
    "12. OperatingSystems\t: continuous variable\n",
    "13. Browser : continuous variable\n",
    "14. Region : continuous variable\n",
    "14. TrafficType : continuous variable\n",
    "14. VisitorType : categorical variable\n",
    "14. Weekend : continuous variable\n",
    "14. Revenue : target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jQaNp6vlIQW"
   },
   "source": [
    "### Splitting the Dataset\n",
    "\n",
    "The original dataset explained above was split into four separate datasets. You are provided only three of these. The fourth is hidden and will be used to test your implementations via the gradescope autograder.\n",
    "\n",
    "**Training Data:** For training decision tree and random forest algorithms in parts 2 and 3.\n",
    "    \n",
    "**Validation Data:** (optional)\n",
    "    \n",
    "**Testing Data:** For testing your decision tree and random forest algorithms in parts 2 and 3.\n",
    "\n",
    "**Hidden Data:** This data will be left out and will instead be used to grade your imlementations on gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zv8fRYkp2yOv"
   },
   "outputs": [],
   "source": [
    "# helper function. You don't have to modify it\n",
    "data_test = pd.read_csv(\"hw4_fall2020_data_test.csv\")\n",
    "data_valid = pd.read_csv(\"hw4_fall2020_data_valid.csv\")\n",
    "data_train = pd.read_csv(\"hw4_fall2020_data_train.csv\")\n",
    "\n",
    "categorical = ['Month']\n",
    "numerical = ['Administrative','Administrative_Duration','Informational',\n",
    "             'Informational_Duration','ProductRelated','ProductRelated_Duration','BounceRates','ExitRates','PageValues','SpecialDay','Month','OperatingSystems','Browser','Region','TrafficType','VisitorType','Weekend']\n",
    " \n",
    "X_train = data_train.drop(columns = 'Revenue')\n",
    "y_train = data_train['Revenue']\n",
    "X_test = data_test.drop(columns = 'Revenue')\n",
    "y_test = data_test['Revenue']\n",
    "X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n",
    "\n",
    "X_valid = data_valid.drop(columns = 'Revenue')\n",
    "y_valid = data_valid['Revenue']\n",
    "X_valid, y_valid = np.array(X_valid), np.array(y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IropaXmi2yOy"
   },
   "source": [
    "Let us train and evaluate the performance of our decision tree on the test set. Note that it is trivially possible to achieve 77% accuracy because of the distribution of \"revenue\" shopping sessions in the dataset. You can use the provided test set to evaluate your implementation, however, your implementation will be tested using a left out hidden test set. You will need to obtain 87% on the hidden test set to receive full credit. Change the default parameter (max_depth) in your MyDecisionTree class to be the ones that you would like to be used in grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaUQz7x52yOy",
    "outputId": "fe6cb811-a827-4f91-b89f-6b14b416d8f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the decision tree\n",
      "accuracy: 0.8024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8024291497975709"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Initializing a decision tree.\n",
    "dt = MyDecisionTree(max_depth=10)\n",
    "\n",
    "# Building a tree\n",
    "print(\"fitting the decision tree\")\n",
    "dt.fit(X_train, y_train, 0)\n",
    "\n",
    "# Evaluating the decision tree\n",
    "dt.DecisionTreeEvalution(X_test,y_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aGIzP6dtY-Y"
   },
   "source": [
    "# Part 3: Random Forests [20pts]\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of examples almost inevitably leads to **overfitting**. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging'). This stems from the idea that a collection of weak learners can learn decision boundaries as well as a strong learner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zP6DnH62yO9"
   },
   "source": [
    "### Part 3.1 Random Forest Implementation (15 pts)\n",
    "\n",
    "\n",
    "A Random Forest is a collection of decision trees, built as follows:\n",
    "\n",
    "1) For every tree we're going to build:\n",
    "\n",
    "    a) Subsample the examples with replacement. Note that in this question, the size of the subsample data is equal to the original dataset. \n",
    "    \n",
    "    b) From the subsamples in a), choose attributes at random to learn on in accordance with a provided attribute subsampling rate. Based on what it was mentioned in the class, we randomly pick features in each split. We use a more general approach here to make the programming part easier. Let's randomly pick some features (70% percent of features) and grow the tree based on the pre-determined randomly selected features. Therefore, there is no need to find random features in each split.\n",
    "    \n",
    "    c) Fit a decision tree to the subsample of data we've chosen to a certain depth.\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "In RandomForests Class, \n",
    "1. X is assumed to be a matrix with num_training rows and num_features columns where num_training is the\n",
    "number of total records and num_features is the number of features of each record. \n",
    "\n",
    "2. y is assumed to be a vector of labels of length num_training.\n",
    "\n",
    "**NOTE:** Lookout for TODOs for the parts that needs to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n8GGVU7tYGh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from decision_tree import MyDecisionTree\n",
    "\n",
    "\"\"\"\n",
    "NOTE: You are required to use your own decision tree MyDecisionTree() to finish random forest.\n",
    "\"\"\"\n",
    "\n",
    "class RandomForest(object):\n",
    "    def __init__(self, n_estimators=12, max_depth=5, max_features=0.95):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initialization done here\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.bootstraps_row_indices = []\n",
    "        self.feature_indices = []\n",
    "        self.out_of_bag = []\n",
    "        self.decision_trees = [MyDecisionTree(max_depth=max_depth) for i in range(n_estimators)]\n",
    "        \n",
    "    def _bootstrapping(self, num_training, num_features, random_seed = None):\n",
    "        \"\"\"\n",
    "        TODO: [5 pts]\n",
    "        - Randomly select a sample dataset of size num_training with replacement from the original dataset. \n",
    "        - Randomly select certain number of features (num_features denotes the total number of features in X, \n",
    "          max_features denotes the percentage of features that are used to fit each decision tree)\n",
    "          without replacement from the total number of features.\n",
    "        \n",
    "        Return:\n",
    "        - row_idx: the row indices corresponding to the row locations of the selected samples in the original dataset.\n",
    "        - col_idx: the column indices corresponding to the column locations of the selected features in the original feature list.\n",
    "        \n",
    "        Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        \"\"\"\n",
    "        # You must set the random seed to pass the bootstrapping unit test, which is already implemented for you.\n",
    "        # Since random_seed is set to None, this function will always be random except for during the bootstrapping unit test\n",
    "        np.random.seed(seed = random_seed)\n",
    "        row_idx = np.random.choice(num_training, num_training, replace=True)\n",
    "        col_idx = np.random.choice(num_features, round(num_features * self.max_features), replace=False)\n",
    "        return row_idx, col_idx\n",
    "\n",
    "            \n",
    "    def bootstrapping(self, num_training, num_features):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.n_estimators):\n",
    "            total = set(list(range(num_training)))\n",
    "            row_idx, col_idx = self._bootstrapping(num_training, num_features)\n",
    "            total = total - set(row_idx)\n",
    "            self.bootstraps_row_indices.append(row_idx)\n",
    "            self.feature_indices.append(col_idx)\n",
    "            self.out_of_bag.append(total)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Train decision trees using the bootstrapped datasets.\n",
    "        Note that you need to use the row indices and column indices.\n",
    "        \n",
    "        Inputs:\n",
    "        X: a matrix with num_training rows and num_features columns where num_training is the number of total records and num_features is the number of features of each record. \n",
    "        y: a vector of labels of length num_training. \n",
    "        \"\"\"\n",
    "        num_training, num_features = np.shape(X)\n",
    "        self.bootstrapping(num_training, num_features)\n",
    "        for x in range(self.n_estimators):\n",
    "            train = X[np.ix_(self.bootstraps_row_indices[x], self.feature_indices[x])]\n",
    "            labels = y[self.bootstraps_row_indices[x]]\n",
    "            self.decision_trees[x].fit(train, labels, self.max_depth)\n",
    "        \n",
    "    \n",
    "    def OOB_score(self, X, y):\n",
    "        # helper function. You don't have to modify it\n",
    "        accuracy = []\n",
    "        for i in range(len(X)):\n",
    "            predictions = []\n",
    "            for t in range(self.n_estimators):\n",
    "                if i in self.out_of_bag[t]:\n",
    "                    predictions.append(self.decision_trees[t].predict(X[i][self.feature_indices[t]]))\n",
    "            if len(predictions) > 0:\n",
    "                accuracy.append(np.sum(predictions == y[i]) / float(len(predictions)))\n",
    "        return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlxXUpNE2yPA"
   },
   "source": [
    "### Part 3.2 Hyperparameter tuning(5pts)\n",
    "\n",
    "Change the hyperparamters below to obtain at least a 88% accuracy on the train dataset. Change the default parameters in your RandomForest class to be the ones that you would like to be used in grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AC1-lWuct2wj"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-c777a756ec82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mrandom_forest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOOB_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-402524630cc6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbootstraps_row_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbootstraps_row_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_trees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-3a6fbc43fd87>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-3a6fbc43fd87>\u001b[0m in \u001b[0;36mbuildTree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# find best feature and attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mbest_split_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_split_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mX_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_split_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_split_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         t.tree = {\n",
      "\u001b[0;32m<ipython-input-70-9cc7cb901116>\u001b[0m in \u001b[0;36mfind_best_feature\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0minfo_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbsf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mbsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mig\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0minfo_gain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0minfo_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-9cc7cb901116>\u001b[0m in \u001b[0;36mfind_best_split\u001b[0;34m(X, y, split_attribute)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mbest_split_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_gain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_attribute\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mx_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_attribute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mig\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0minfo_gain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-9cc7cb901116>\u001b[0m in \u001b[0;36mpartition_classes\u001b[0;34m(X, y, split_attribute, split_val)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mX_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0my_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0my_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-9cc7cb901116>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mX_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0my_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0my_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: \n",
    "n_estimators defines how many decision trees are fitted for the random forest (at least 10). \n",
    "max_depth defines a stop condition when the tree reaches to a certain depth.\n",
    "max_features controls the percentage of features that are used to fit each decision tree.\n",
    "Tune these three parameters to achieve a better accuracy. You will need to obtain 88% on the \n",
    "train set to receive full credit.\n",
    "The random forest fitting may take 5 - 25 minutes. We will not take running time into account when grading\n",
    "this part, however, you need to make sure that the gradescope autograder does not time out.\n",
    "\"\"\"\n",
    "\n",
    "# Hint: the range of n_estimators should be from 8 to 12.\n",
    "#       the range of max_depth should be from 3 to 5\n",
    "#       the range of max_features should be from 0.85 to 0.95\n",
    "# Feel free to explore more!\n",
    "n_estimators = 12\n",
    "max_depth = 5\n",
    "max_features = 0.95\n",
    "\n",
    "random_forest = RandomForest(n_estimators, max_depth, max_features)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "    \n",
    "accuracy=random_forest.OOB_score(X_train, y_train)\n",
    "print(\"accuracy: %.4f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: SVM [40 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Fitting a SVM classifier by hand (10 Pts)\n",
    "\n",
    "Consider a dataset with 2 points in 1-dimensional space: $(x_1 = -2, t_1 = 1)$ and $(x_2 = 1, t_2 = 1)$. Here $x$ are the point coordinates (a single feature, i.e. scalar) and $t$ are the classes.\n",
    " \n",
    "Consider mapping each point to 3-dimensional space using the feature vector $\\phi(x) = [1,2x, x^2]$. (This is equivalent to using a second order polynomial kernel.) The max margin classifier has the form\n",
    " \n",
    "$$\\min ||\\mathbf{w}||^2 $$\n",
    " \n",
    "$$s.t.$$\n",
    " \n",
    "$$t_1(\\mathbf{w}^T \\phi(x_1) + b)  1 $$\n",
    " \n",
    "$$t_2(\\mathbf{w}^T \\phi(x_2)+ b)  1 $$\n",
    " \n",
    "**Hint:** $\\phi(x_1)$ and $\\phi(x_2)$ are the suppport vectors. We have already given you the solution for the suppport vectors and you need to calculate back the parameters. Margin is equal to $\\frac{1}{||\\mathbf{w}||}$ and half margin is equal to $\\frac{2}{||\\mathbf{w}||}$.\n",
    " \n",
    "(1) Find a vector parallel to the optimal vector $\\mathbf{w}$. (2pts)\n",
    " \n",
    "(2) Calculate the value of the margin achieved by this $\\mathbf{w}$? (2pts)\n",
    " \n",
    "(3) Solve for $\\mathbf{w}$, given that the margin is equal to $1/||\\mathbf{w}||$. (2pts)\n",
    " \n",
    "(4) Solve for $b$ using your value for $\\mathbf{w}$. (2pts)\n",
    " \n",
    "(5) Write down the form of the discriminant function $f(x) = \\mathbf{w}^T\\phi(x)+b$ as an explicit function of $x$. (2pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 SVM Kernels (10 Pts)\n",
    "\n",
    "(1) (5 points) Given valid kernels $k_1(\\mathbf{x}, \\mathbf{x}')$ and $k_2(\\mathbf{x}, \\mathbf{x}')$, prove that the following new kernels will also\n",
    "be valid: \n",
    " \n",
    "a. $K(\\mathbf{x}, \\mathbf{z}) = k_1(f(\\mathbf{x}), f(\\mathbf{z})), where f: X  X $   \n",
    " \n",
    "b. $K(\\mathbf{x}, \\mathbf{z}) = k_1(\\mathbf{x}, \\mathbf{z}) + k_2(\\mathbf{x}, \\mathbf{z})$  \n",
    " \n",
    "(2) (5 points)  \n",
    "a. Consider the polynomial kernel $$K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^T\\mathbf{y})^q$$ with q=2. Let $\\mathbf{x}, \\mathbf{y} \\in R^3$ for simplicity. Define one calculation as one multiplication, addition or square operation. Assume that constants (like $\\sqrt{2}$) are already calculated and given. Count the calculations after simplifying the terms.\n",
    " \n",
    "a. What is the number of calculations required to find $K(\\mathbf{x}, \\mathbf{y})$ through direct computation?  (1 points)  \n",
    "b. Can you find the corresponding feature mapping $\\phi(\\mathbf{x})$?   (1 points)  \n",
    "c. What is the number of calculations required for calculating the above feature map for a scalar $x$ ?   (1 points)  \n",
    "d. What is the number of calculations to find $K(\\mathbf{x}, \\mathbf{y})$ using $\\phi(\\mathbf{x})^T \\phi(\\mathbf{y})$? Comment on this with respect to your answer in (a).   (2 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SVM Implementation (20 Pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the following instructions carefully and also read the descriptions given in function docstring.\n",
    " \n",
    "In this part, you will implement your own SVM classifier and do some feature engineering to work with non-linear decision boundaries.\n",
    " \n",
    "The \"reg\" parameter passed to the class is to be used with the hinge loss i.e. \n",
    "$$\n",
    "\\begin{equation}\n",
    "       \\text{hinge_loss} = \\text{reg} \\cdot \\frac{1}{N} \\sum_{n=1}^N \\max(0, 1 - t_n(\\mathbf{w}^T\\mathbf{x_n}))\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "And similarly, should also be included in the gradient used in update rule for $\\mathbf{w}$.\n",
    " \n",
    "The maximum number of epochs is set to 5000 but the model shouldn't take that many epochs to converge. You should check if the update changes cost by more than previous cost times the eps parameter and stop the iteration if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Synthetic Dataset')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Here we create random data samples\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers=2, random_state=0, cluster_std=0.8)\n",
    "y[y==0]=-1\n",
    "f, ax = plt.subplots(nrows=1, ncols=1,figsize=(5,5))\n",
    "\n",
    "plt.scatter(X[y==1,0],X[y==1,1], color='steelblue', label='1')\n",
    "plt.scatter(X[y==-1,0],X[y==-1,1], color='tomato', label='-1')\n",
    "plt.legend()\n",
    "ax.set_title(\"Synthetic Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "    def __init__(self, reg = 1000, lr = 1e-8, eps=0.01, print_every=20):\n",
    "        '''\n",
    "        You don't have to modify this function\n",
    "        Initialization done here\n",
    "        '''\n",
    "        \n",
    "        self.W = None\n",
    "        self.reg = reg\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.print_every = 20\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        '''\n",
    "        TODO - Compute the total cost function for SVM\n",
    "        \n",
    "        returns:\n",
    "            cost - float, total value of cost function\n",
    "        [4 points]\n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "    def calculate_cost_gradient(self, X_point, y_point):\n",
    "        '''\n",
    "        TODO - Compute the gradient of the cost function for SVM\n",
    "        \n",
    "        returns:\n",
    "            dW - (D, ) array of piecewise gradient of SVM loss \n",
    "                 function\n",
    "        [4 points]\n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        TODO - Use SGD to fit W to data using SVM update rule.\n",
    "        \n",
    "        You should use calculate_cost_gradient function\n",
    "        to get the gradient for update rule and compute_cost\n",
    "        function to get cost which can be displayed after \n",
    "        every few epochs. \n",
    "        \n",
    "        features doesn't include 1's for bias term. It \n",
    "        needs to be separately accounted for in your code.\n",
    "        \n",
    "        Use eps, print_every and lr defined in __init__()\n",
    "        \n",
    "        Update self.W directly.\n",
    "        \n",
    "        returns: None\n",
    "        [4 points]\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "       \n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        TODO - Predict y_test from W updated in fit function.\n",
    "        \n",
    "        X_test doesn't include 1's for bias term. It \n",
    "        needs to be separately accounted for in your code.\n",
    "        \n",
    "        returns: \n",
    "            y_test_predicted: (N, ) array predicted by model \n",
    "                              with values from {-1, 1} \n",
    "        [4 points]\n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is:20 and Cost is: 557.546842419189\n",
      "Epoch is:40 and Cost is: 421.0288820825817\n",
      "Epoch is:60 and Cost is: 312.7820985636548\n",
      "Epoch is:80 and Cost is: 245.15037250519174\n",
      "Epoch is:100 and Cost is: 209.13765371471908\n",
      "Epoch is:120 and Cost is: 186.54370158789197\n",
      "Epoch is:140 and Cost is: 171.78817340774256\n",
      "Epoch is:160 and Cost is: 160.96506598867512\n",
      "Epoch is:180 and Cost is: 152.5768547105975\n",
      "Epoch is:200 and Cost is: 145.73564849571147\n",
      "Epoch is:220 and Cost is: 139.98259087485098\n",
      "Epoch is:240 and Cost is: 135.39561255240955\n",
      "Epoch is:260 and Cost is: 131.60795354460987\n",
      "Epoch is:280 and Cost is: 128.36467745524578\n",
      "Epoch is:300 and Cost is: 125.40099739935914\n",
      "Epoch is:320 and Cost is: 122.66592930361843\n",
      "Epoch is:340 and Cost is: 120.39185844639974\n",
      "Epoch is:360 and Cost is: 118.32804266780508\n",
      "Epoch is:380 and Cost is: 116.40785559530342\n",
      "Epoch is:400 and Cost is: 114.69035282651396\n",
      "Epoch is:420 and Cost is: 113.11505219432158\n",
      "Epoch is:440 and Cost is: 111.62869535192029\n",
      "Epoch is:460 and Cost is: 110.19008487923094\n",
      "Epoch is:480 and Cost is: 108.82924280794697\n",
      "Epoch is:500 and Cost is: 107.5684060975225\n",
      "Epoch is:520 and Cost is: 106.40823808315085\n",
      "Epoch is:540 and Cost is: 105.35769405996042\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE\n",
    "# Train the SVM classifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "svm_cls = SVM()\n",
    "svm_cls.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 0.96\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE\n",
    "# Check test accuracy\n",
    "# You should get more than 90% accuracy\n",
    "# [4 points] If accuracy better than 90%\n",
    "y_test_predicted = svm_cls.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on test dataset: {}\".format(accuracy_score(y_test, \n",
    "                                                           y_test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "def visualize_decision_boundary(X, y, feature_new=None, h=0.02):\n",
    "    '''\n",
    "    Function to vizualize decision boundary\n",
    "    feature_new is a function to get X with additional features\n",
    "    '''\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx_1, xx_2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
    "                         np.arange(x2_min, x2_max, h))\n",
    "\n",
    "    if X.shape[1] == 2:\n",
    "        Z = svm_cls.predict(np.c_[xx_1.ravel(), xx_2.ravel()])\n",
    "    else:\n",
    "        X_conc = np.c_[xx_1.ravel(), xx_2.ravel()]\n",
    "        X_new = feature_new(X_conc)\n",
    "        Z = svm_cls.predict(X_new)\n",
    "\n",
    "    Z = Z.reshape(xx_1.shape)\n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=1,figsize=(5,5))\n",
    "    plt.contourf(xx_1, xx_2, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('X_1')\n",
    "    plt.ylabel('X_2')\n",
    "    plt.xlim(xx_1.min(), xx_1.max())\n",
    "    plt.ylim(xx_2.min(), xx_2.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DO NOT CHANGE\n",
    "# Visualize decision boundary\n",
    "\n",
    "visualize_decision_boundary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Two Layer Neural Network (30 Bonus Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "![Perceptron](https://drive.google.com/uc?id=1j7-S5P1BKTkV_-FYFokUVSqcRKnrBObf)\n",
    "\n",
    "The perceptron is a basic function that mimics the human neuron. A single perceptron can be thought of as a linear hyperplane as in SVMs followed by a non-linear function. $$a_{i} = \\phi \\left( \\sum \\limits_{j=1}^{n} w_{ij}x_{j}+b_{i} \\right) = \\phi(w_{i}^{T}x+b_{i})$$ where $w_{i} \\in R^{n}$ is the weight vector, $x \\in R^{n}$ is ONE data point with $n$ features, $b_{i} \\in R$ is the bias element, and $\\phi(.)$ is any non linear function that will be described later. \n",
    "\n",
    "\n",
    "## Fully-connected Layer\n",
    "Typically, a modern neural network contains millions of perceptrons as the one shown in the previous image. Preceptrons interact in different configurations. Such as cascaded or parallel. In this part we describe a fully connected layer configuration in a neural network, which uses parallel perceptrons to form a layer. \n",
    "\n",
    "We extend the previous notation to describe a fully connected layer as follows\n",
    "$$a = \\phi(Z), \\quad Z=Wx+b$$ where $Z \\in R^{m}$ is the output vector after appling linear operations, $W \\in R^{m \\times n}$ is the weight matrix, and $b \\in R^{m}$ is the bais vector. \n",
    "\n",
    "Therefore, we can use the perceptron layer to update the data signal. The whole operation can be summarized as,\n",
    "$$a^{[l]} = \\phi(Wa^{[l-1]}+b) $$ where $a^{[l-1]}$ is the output of the previous layer as shown in figure below. Since we are going to build a two layer neural network model, $l \\in \\{1, 2\\}$ in this problem.\n",
    "\n",
    "![Fully connected layer](https://drive.google.com/uc?id=1nDWpqhk_9zqNBPpGI3y94YerpXJub_Xk)\n",
    "\n",
    "\n",
    "## Activation Function\n",
    "There are many kinds of activation function. For this question, we are going to use Relu and Sigmoid.\n",
    "### ReLU\n",
    "The Rectified Linear Unit (ReLU) is one of the most commonly used activation function in neural network models. The function is $\\phi(Z)=max(0,Z)$. ReLU shares a lot of the properties of linear functions and it tends to work well on most of the problems. The only issue is that the derivative is not defined at z = 0, which we can overcome by assigning the derivative to 0 at z = 0. However, this means that for z  0 the gradient is zero and again cant learn.\n",
    "### Sigmoid\n",
    "The sigmoid function is another non-linear function with S-shaped curve. This function is useful in the case of binary classification as its output is between 0 and 1. The mathematical form of the function is $\\phi(Z)=\\frac{1}{1+e^{-Z}}$. One of the main disadvantages for using sigmoid function on hidden layers is that the gradient is very close to zero over a large portion of its domain which makes it slow and harder for the learning algorithm to learn.\n",
    "\n",
    "![Relu](https://drive.google.com/uc?id=10g4b7WTbt9Y9QQQ9EJhiqXhQvfEpiXLe)\n",
    "\n",
    "## Cross Entropy Loss\n",
    "An essential piece in training a neural network, is the loss function. The whole purpose of gradient decent algorithm is to find some network parameters that minimizes the loss function. In this problem, we minimize Cross Entropy (CE) loss that represents the distance between true data distribution and estimated distribution by neural network. During training of the neural network, we will be looking for network parameters that minimizes this distance. The mathematical form of the CE loss is given by \n",
    "$$CE(p,q) = -\\sum\\limits_{i} p(x_{i})\\log q(x_{i}) $$\n",
    "where $p(x)$ is the true distribution and $q(x)$ is the estimated distribution. \n",
    "### Implementation details\n",
    "For binary classification problems, we have probability distribution of a label $y_{i}$:\n",
    "\\begin{equation}\n",
    "y_{i}= \n",
    "\\begin{cases}\n",
    "&1& \\text{ with probability } p(x_{i}) \\\\\n",
    "&0& \\text{ with probability } 1- p(x_{i})\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "A frequentist estimate of $p(x_{i})$ can be written as $p(x_{i})= \\sum\\limits_{i=1}^{N} \\frac{y_{i}}{N}$. Therefore the cross entropy for binary estimation can be written as \n",
    "$$CE(y_{i},\\hat{y_{i}}) = -\\frac{1}{N}\\sum\\limits_{i=1}^{N}[y_{i} \\log (\\hat{y_{i}}) +(1-y_{i}) \\log (1-\\hat{y_{i}})]$$\n",
    "where $y_{i} \\in \\{ 0,1\\}$ is the true label and $\\hat{y_{i}} \\in \\{0,1\\}$ is the estimated distribution.  \n",
    "\n",
    "## Forward Propagation\n",
    "We start by intializing the weights of the fully connected layer using Xavier initialization [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). During training we pass all the data points through the network layer by layer.\n",
    "\\begin{eqnarray}\n",
    "a^{[0]} &=& x\\\\\n",
    "Z^{[1]}&=& W^{[1]}a^{[0]}+b^{[1]} \\\\\n",
    "a^{[1]}&=& Relu(Z^{[1]}) \\\\\n",
    "Z^{[2]}&=& W^{[2]}a^{[1]}+b^{[2]} \\\\\n",
    "\\hat{y}=a^{[2]}&=& Sigmoid(Z^{[2]}) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Then we get the output and compute the loss \n",
    "$$l = CE(y,\\hat{y}) = -\\frac{1}{N}(y \\log (\\hat{y}) +(1-y) \\log (1-\\hat{y}))$$\n",
    "\n",
    "## Backward propagation\n",
    "After the forward pass, we do back propagation to update the weights and biases in the direction of the negative gradient of the loss function. So we update the weights and biases using the following formulas\n",
    "\\begin{equation}\n",
    "W^{[2]} := W^{[2]} - lr \\times \\frac{\\partial l}{\\partial W^{[2]}} \\\\\n",
    "b^{[2]} := b^{[2]} - lr \\times \\frac{\\partial l}{\\partial b^{[2]}} \\\\\n",
    "W^{[1]} := W^{[1]} - lr \\times \\frac{\\partial l}{\\partial W^{[1]}} \\\\\n",
    "b^{[1]} := b^{[1]} - lr \\times \\frac{\\partial l}{\\partial b^{[1]}}\n",
    "\\end{equation}\n",
    "where $lr$ is the learning rate. \n",
    "\n",
    "\n",
    "\n",
    "To compute the terms $\\frac{\\partial l}{\\partial W^{[i]}}$ and $ \\frac{\\partial l}{\\partial b^{[i]}}$ \n",
    "we use chain rule:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial l}{\\partial W^{[2]}}&=&\\frac{\\partial l}{\\partial a^{[2]}}\\frac{\\partial a^{[2]}}{\\partial Z^{[2]}}\\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} \\\\\n",
    "\\frac{\\partial l}{\\partial b^{[2]}}&=&\\frac{\\partial l}{\\partial a^{[2]}}\\frac{\\partial a^{[2]}}{\\partial Z^{[2]}}\\frac{\\partial Z^{[2]}}{\\partial b^{[2]}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "$\\frac{\\partial l}{\\partial a^{[2]}}$ is the differentiation of the cross entropy function at point $a^{[2]}$, $\\frac{\\partial a^{[2]}}{\\partial Z^{[2]}}$ is the differentiation of the Sigmoid function at point $Z^{[2]}$, $\\frac{\\partial Z^{[2]}}{\\partial W^{[2]}}$ is equal to $a^{[1]}$, and $\\frac{\\partial Z^{[2]}}{\\partial b^{[2]}}$ is equal to $1$. To compute $\\frac{\\partial l}{\\partial W^{[2]}}$, we need $a^{[2]}, Z^{[2]}, a^{[1]}$ calculated during forward propagation. Therefore, we need to store these values during the forward propagation to be able to access them during bacward propagation. The functional form of the CE differentiation and Sigmoid differentiation are given by \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial l}{\\partial a^{[2]}} &=& \\frac{-1}{N}\\left(\\frac{y}{a^{[2]}}-\\frac{1-y}{1-a^{[2]}}\\right) \\\\\n",
    "\\frac{\\partial a^{[2]}}{\\partial Z^{[2]}} &=& \\frac{1}{1+e^{-Z^{[2]}}} \\left(1- \\frac{1}{1+e^{-Z^{[2]}}} \\right) \\\\\n",
    "\\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} &=& a^{[1]} \\\\\n",
    "\\frac{\\partial Z^{[2]}}{\\partial b^{[2]}} &=& 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "Similarly,\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial l}{\\partial W^{[1]}}&=&\\frac{\\partial l}{\\partial a^{[2]}}\\frac{\\partial a^{[2]}}{\\partial Z^{[2]}}\\frac{\\partial Z^{[2]}}{\\partial a^{[1]}} \\frac{a^{[1]}}{Z^{[1]}} \\frac{Z^{[1]}}{W^{[1]}}  \\\\\n",
    "\\frac{\\partial l}{\\partial b^{[1]}}&=&\\frac{\\partial l}{\\partial a^{[2]}}\\frac{\\partial a^{[2]}}{\\partial Z^{[2]}}\\frac{\\partial Z^{[2]}}{\\partial a^{[1]}} \\frac{a^{[1]}}{Z^{[1]}} \\frac{Z^{[1]}}{b^{[1]}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Where \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial Z^{[2]}}{\\partial a^{[1]}} &=& W^{[1]} \\\\\n",
    "\\frac{\\partial a^{[1]}}{\\partial Z^{[1]}} &=&  \n",
    "\\begin{cases}\n",
    "&0& \\text{ if } Z^{[1]} \\leq 0 \\\\\n",
    "&1& \\text{ if } Z^{[1]} > 0 \n",
    "\\end{cases}\\\\\n",
    "\\frac{\\partial Z^{[1]}}{\\partial W^{[1]}} &=& x\\\\\n",
    "\\frac{\\partial Z^{[1]}}{\\partial b^{[1]}} &=& 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that $\\frac{\\partial a^{[1]}}{\\partial Z^{[1]}}$ is the differentiation of the Relu function at $Z^{[1]}$.\n",
    "\n",
    "Now, let's try to implememt this two layer neural network model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We are going to use Breast Cancer Wisconsin (Diagnostic) Data Set provided by sklearn\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "to train a 2 fully connected layer neural net. We are going to buld the neural network from scratch. \n",
    "'''\n",
    "\n",
    "\n",
    "class dlnet:\n",
    "\n",
    "    def __init__(self, x, y, lr=0.003):\n",
    "        '''\n",
    "        This method initializes the class, its implemented for you.\n",
    "        Args:\n",
    "            x: data\n",
    "            y: labels\n",
    "            Yh: predicted labels\n",
    "            dims: dimensions of different layers\n",
    "            param: dictionary of different layers parameters\n",
    "            ch: Cache dictionary to store forward parameters that are used in backpropagation\n",
    "            loss: list to store loss values\n",
    "            lr: learning rate\n",
    "            sam: number of training samples we have\n",
    "\n",
    "        '''\n",
    "        self.X = x  # features\n",
    "        self.Y = y  # ground truth labels\n",
    "\n",
    "        self.Yh = np.zeros((1, self.Y.shape[1]))  # estimated labels\n",
    "        self.dims = [30, 15, 1]  # dimensions of different layers\n",
    "\n",
    "        self.param = {}  # dictionary for different layer variables\n",
    "        self.ch = {}  # cache variable\n",
    "        self.loss = []\n",
    "\n",
    "        self.lr = lr  # learning rate\n",
    "        self.sam = self.Y.shape[1]  # number of training samples we have\n",
    "        self._estimator_type = 'classifier'\n",
    "\n",
    "    def nInit(self):\n",
    "        '''\n",
    "        This method initializes the neural network variables, it's already implemented for you.\n",
    "        Check it and relate to mathematical the description above.\n",
    "        You are going to use these variables in forward and backward propagation.\n",
    "        '''\n",
    "        np.random.seed(1)\n",
    "        self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0])\n",
    "        self.param['b1'] = np.zeros((self.dims[1], 1))\n",
    "        self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1])\n",
    "        self.param['b2'] = np.zeros((self.dims[2], 1))\n",
    "        return\n",
    "\n",
    "    def Relu(self, Z):\n",
    "        '''\n",
    "        In this method you are going to implement element wise Relu.\n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension.\n",
    "        Input: Z of any dimension\n",
    "        return: Relu(Z)\n",
    "        [2 points]\n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def Sigmoid(self, Z):\n",
    "        '''\n",
    "        In this method you are going to implement element wise Sigmoid.\n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension.\n",
    "        Input: Z of any dimension\n",
    "        return: Sigmoid(Z)\n",
    "        [2 points]\n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def dRelu(self, Z):\n",
    "        '''\n",
    "        In this method you are going to implement element wise differentiation of Relu.\n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension.\n",
    "        Input: Z of any dimension\n",
    "        return: dRelu(Z)\n",
    "        [3 points]\n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def dSigmoid(self, Z):\n",
    "        '''\n",
    "        In this method you are going to implement element wise differentiation of Sigmoid.\n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension.\n",
    "        Input: Z of any dimension\n",
    "        return: dSigmoid(Z)\n",
    "        [3 points]\n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def nloss(self, y, yh):\n",
    "        '''\n",
    "        In this method you are going to implement Cross Entropy loss.\n",
    "        Refer to the description above and implement the appropriate mathematical equation.\n",
    "        Input: y 1xN: ground truth labels\n",
    "               yh 1xN: neural network output after Sigmoid\n",
    "\n",
    "        return: CE 1x1: loss value\n",
    "        [3 points]\n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Fill in the missing code lines, please refer to the description for more details.\n",
    "        Check nInit method and use variables from there as well as other implemeted methods.\n",
    "        Refer to the description above and implement the appropriate mathematical equations.\n",
    "        donot change the lines followed by #keep.\n",
    "        [7 points]\n",
    "        '''\n",
    "        #Todo: uncomment the following 7 lines and complete the missing code\n",
    "        \n",
    "        #Z1 =  \n",
    "        #A1 = \n",
    "        #self.ch['Z1'], self.ch['A1']=Z1, A1 #keep \n",
    "\n",
    "        #Z2 = \n",
    "        #A2 = \n",
    "        #self.ch['Z2'], self.ch['A2']=Z2, A2 #keep\n",
    "        #return A2 #keep\n",
    "\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, y, yh):\n",
    "        '''\n",
    "        Fill in the missing code lines, please refer to the description for more details\n",
    "        You will need to use cache variables, some of the implemeted methods, and other variables as well\n",
    "        Refer to the description above and implement the appropriate mathematical equations.\n",
    "        do not change the lines followed by #keep.\n",
    "        [10 points]\n",
    "        '''\n",
    "        #Todo: uncomment the following 13 lines and complete the missing part\n",
    "\n",
    "        # dLoss_o2 =\n",
    "        # dLoss_u2 =\n",
    "        # dLoss_W2 =\n",
    "        # dLoss_b2 =\n",
    "        # dLoss_o1 =\n",
    "        # dLoss_u1 =\n",
    "        # dLoss_W1 =\n",
    "        # dLoss_b1 =\n",
    "        # self.param[\"W2\"] = self.param[\"W2\"] - self.lr * dLoss_W2  # keep\n",
    "        # self.param[\"b2\"] = self.param[\"b2\"] - self.lr * dLoss_b2  # keep\n",
    "        # self.param[\"W1\"] = self.param[\"W1\"] - self.lr * dLoss_W1  # keep\n",
    "        # self.param[\"b1\"] = self.param[\"b1\"] - self.lr * dLoss_b1  # keep\n",
    "        #return dLoss_W2, dLoss_b2, dLoss_W1, dLoss_b1 #keep\n",
    "\n",
    "\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def gradient_decent(self, x, y, iter=60000):\n",
    "        '''\n",
    "        This function is an implementation of the gradient decent algorithm,\n",
    "        Its implemented for you.\n",
    "        '''\n",
    "        self.nInit()\n",
    "        for i in range(0, iter):\n",
    "            yh = self.forward(x)\n",
    "            loss = self.nloss(y, yh)\n",
    "            dLoss_W2, dLoss_b2, dLoss_W1, dLoss_b1 = self.backward(y, yh)\n",
    "            self.loss.append(loss)\n",
    "            if i % 2000 == 0: print(\"Loss after iteration %i: %f\" % (i, loss))\n",
    "        return\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        This function predicts new data points\n",
    "        Its implemented for you\n",
    "        '''\n",
    "        Yh = self.forward(x)\n",
    "        return np.round(Yh).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.699796\n",
      "Loss after iteration 2000: 0.061023\n",
      "Loss after iteration 4000: 0.052467\n",
      "Loss after iteration 6000: 0.048680\n",
      "Loss after iteration 8000: 0.046025\n",
      "Loss after iteration 10000: 0.043946\n",
      "Loss after iteration 12000: 0.042218\n",
      "Loss after iteration 14000: 0.040690\n",
      "Loss after iteration 16000: 0.039277\n",
      "Loss after iteration 18000: 0.037926\n",
      "Loss after iteration 20000: 0.036604\n",
      "Loss after iteration 22000: 0.035275\n",
      "Loss after iteration 24000: 0.033909\n",
      "Loss after iteration 26000: 0.032485\n",
      "Loss after iteration 28000: 0.030986\n",
      "Loss after iteration 30000: 0.029403\n",
      "Loss after iteration 32000: 0.027737\n",
      "Loss after iteration 34000: 0.026004\n",
      "Loss after iteration 36000: 0.024228\n",
      "Loss after iteration 38000: 0.022442\n",
      "Loss after iteration 40000: 0.020309\n",
      "Loss after iteration 42000: 0.018405\n",
      "Loss after iteration 44000: 0.016643\n",
      "Loss after iteration 46000: 0.015034\n",
      "Loss after iteration 48000: 0.013588\n",
      "Loss after iteration 50000: 0.012302\n",
      "Loss after iteration 52000: 0.011156\n",
      "Loss after iteration 54000: 0.009915\n",
      "Loss after iteration 56000: 0.008919\n",
      "Loss after iteration 58000: 0.008131\n",
      "Loss after iteration 60000: 0.007325\n",
      "Loss after iteration 62000: 0.006696\n",
      "Loss after iteration 64000: 0.006167\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a0697f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Training the Neural Network, you donot need to modify this cell\n",
    "We are going to use Breast Cancer Wisconsin (Diagnostic) Data Set provided by sklearn\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "'''\n",
    "dataset = load_breast_cancer() # load the dataset\n",
    "x, y = dataset.data, dataset.target\n",
    "x = MinMaxScaler().fit_transform(x) #normalize data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1) #split data\n",
    "x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.reshape(1,-1), y_test #condition data\n",
    "\n",
    "nn = dlnet(x_train,y_train,lr=0.1) # initalize neural net class\n",
    "nn.gradient_decent(x_train, y_train, iter = 66000) #train\n",
    "\n",
    "# create figure\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title('Training')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  malignant       0.96      0.95      0.95        55\n",
      "     benign       0.97      0.98      0.97        88\n",
      "\n",
      "avg / total       0.97      0.97      0.96       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing Neural Network\n",
    "'''\n",
    "y_predicted = nn.predict(x_test) # predict \n",
    "\n",
    "#plot\n",
    "print(classification_report(y_test, y_predicted, target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_Summer2020_solutions_v1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
